---
title: 'Muon Optimizer'
date: 2025-12-18
permalink: /posts/2025/12/moun_optimizer/
tags:
  - LLM
  - Open Source
  - Training
  - Optimizer
---

# Technical Deep Dive: The Muon Optimizer
The Muon optimizer represents a paradigm shift from element-wise adaptive methods to matrix-wise orthogonalization methods. To understand its value, we must first rigorously define the baselines and the specific linear algebra problems Muon attempts to solve.

## 1. Revisiting Adam: The Baseline

Adam (Adaptive Moment Estimation) is the standard for training Transformers. It relies on first and second moments to adapt the learning rate for each parameter individually. For a parameter vector $\theta$ and gradient $g_t = \nabla_\theta L(\theta_{t-1})$, the update rules are:

1.  **Momentum (First Moment):**
    $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
2.  **Variance (Second Moment - Uncentered):**
    $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
    *(Note: $g_t^2$ is the element-wise square)*
3.  **Bias Correction:**
    $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
4.  **Parameter Update:**
    $$\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

### The Limitation
Adam rescales updates based on the magnitude of gradients coordinate-wise. However, in deep linear networks (like the interior of Transformer blocks), parameters are naturally **matrices**, not vectors. Treating a matrix as a flat vector destroys the geometric relationship between its rows and columns.

## 2. The Muon Intuition: Why Orthogonalize?

Muon replaces the coordinate-wise scaling of Adam with a matrix-wise operation: **Orthogonalization**.

To understand why, let's decompose the Momentum matrix $M$ using Linear Algebra. Any matrix can be viewed as having two distinct instructions:
1.  **Rotation ($U, V^T$):** "Which direction should we move in?" (The relationships between inputs and outputs).
2.  **Scaling ($S$):** "How far should we move in those directions?" (The importance/strength of the connection).

### The Problem with $M$
In standard training, the Scaling instruction ($S$) is often incredibly unbalanced. The momentum matrix usually has a few massive singular values (dominant directions) and many tiny ones (rare features).

* If we update using just $M$, the optimizer takes huge steps in the dominant directions and almost zero steps in the subtle directions.
* **Analogy:** Imagine hiking down a mountain. $M$ tells you: "Move 100 meters South and 1 centimeter West." You will effectively only move South, ignoring the Westward path even if that is where the true valley lies.

### The Solution: "Rotation Only"
We want to keep the Rotation (the directional advice) because that tells us where the loss decreases. However, we want to discard the Scaling bias. By orthogonalizing $M$, we force all singular values to be **1**.

* **Uniform Updates:** We apply uniform updates to all spectral directions.
* **Pure Rotation:** The update becomes a pure rotation transform applied to the weights.
* **Result:** The optimizer applies "unit energy" to every relevant feature dimension, ensuring rare features are learned just as quickly as dominant ones.


## 3. The Math: The Orthogonal Procrustes Problem

To rigorously define what we are doing, we frame this as an optimization problem. We want to find a matrix $O$ that is as close as possible to our current Momentum matrix $M$.

The formal objective function is:

$$\min_{O \in \mathbb{R}^{n \times m}} || O - M ||_F^2 \quad \text{subject to } O^T O = I$$

### Why the "Subject To" condition?
You might ask: *Why do we need a constraint? Why not just find the $O$ that minimizes the distance?*

If we removed the "subject to" condition, the answer would be trivial: the matrix closest to $M$ is simply $M$ itself! The distance would be zero.

The constraint $O^T O = I$ is the mathematical definition of an **Orthogonal Matrix**.
* It forces the column vectors of $O$ to be of unit length (norm = 1).
* It forces the columns to be perpendicular to each other (dot product = 0).

By adding this constraint, we are telling the math: "Don't just give me *any* matrix. Search **only** within the specific set of Orthogonal Matrices and find the one that aligns best with $M$." This forces the solution to discard the scaling (magnitude) of $M$ while preserving its rotation (direction).

## 4. The Engine: Newton-Schulz Iteration

We know that **SVD** ($M = USV^T$) gives us the perfect theoretical solution ($O = UV^T$). However, calculating SVD is computationally expensive—cubic complexity $O(N^3)$—and hard to parallelize efficiently on GPUs.

This is where **Newton-Schulz Iteration** comes in. It is a numerical method used to approximate the function of a matrix using only matrix multiplication and addition.

### The "Odd Polynomial" Formula
To iteratively transform the singular values of our matrix toward 1 (orthogonalization), Muon uses a specific **Odd Polynomial**:

$$\phi(X) = aX + b(XX^T)X + c(XX^T)^2X$$

In terms of singular values $\sigma$, this corresponds to the scalar polynomial:
$$P(\sigma) = a\sigma + b\sigma^3 + c\sigma^5$$

### Derivation: Why this specific equation?
This equation is not arbitrary. It is derived from two strict mathematical requirements: **Structure Preservation** and **Convergence**.

#### 1. Structure Preservation (Why it looks like that)
We operate on a matrix $M = USV^T$. We want to change $S$ to $I$, but we **must not touch** $U$ and $V^T$ (the rotations).
If we look at the term $M M^T$:
$$M M^T = (U S V^T) (V S U^T) = U S^2 U^T$$
*(Note how $V$ disappeared because $V^T V = I$)*

When we multiply this back by $M$:
$$(M M^T) M = (U S^2 U^T) (U S V^T) = U S^3 V^T$$

The result is the original matrix, but with the singular values cubed ($S^3$). This proves that only polynomials of the form $M(M^T M)^k$ can modify singular values while perfectly preserving the $U$ and $V$ direction matrices.

#### 2. Sign Preservation (Why "Odd"?)
We use an **odd** polynomial (where $f(-x) = -f(x)$) because it preserves the **sign** of the eigenvalues/singular values.
* If we used an *even* function (like $x^2$), we would square the values, turning negative correlations into positive ones. This would flip the direction of our gradient update, causing the model to climb *up* the error mountain!
* An odd polynomial stretches or shrinks the magnitude (pushing values to 1) while strictly respecting the original orientation.

#### 3. The Coefficients (Why these numbers?)
Muon typically uses a standard 5-iteration loop with hardcoded coefficients:
$$a = 3.4445, \quad b = -4.7775, \quad c = 2.0315$$

These coefficients are derived from a 5th-order approximation of Newton's Method. They define a "hyper-optimized" S-curve that:
1.  Has a **Fixed Point** at 1 ($P(1) = 1$).
2.  Has a **Derivative of 0** at 1 (flat top for stability).
3.  Steeply pushes low values up and pushes high values down.

This forces the singular value matrix $S$ to converge to the Identity matrix $I$ within just 5 iterations.

## 5. The Complete Algorithm

Here is the full Muon update loop for a single step:

### 1. Gradient Computation:
$$G_t = \nabla L$$

### 2. Momentum Update:
$$M_t = \beta M_{t-1} + G_t$$

### 3. Normalization (Pre-conditioning):
To ensure the singular values fit within the convergent range of our polynomial, we normalize $M$ by its Frobenius norm:
$$X_0 = \frac{M_t}{\max(1, ||M_t||_F / \sqrt{d_{row} d_{col}})}$$

### 4. Newton-Schulz Loop (Orthogonalization):
For $k = 1 \dots 5$:
$$A = X_{k-1} X_{k-1}^T$$
$$B = A \cdot X_{k-1}$$
$$X_k = a X_{k-1} + b B + c A \cdot B$$

### 5. Parameter Update:
$$\theta_t = \theta_{t-1} - \eta \cdot X_5$$

By using $X_5$ (our approximation of $O$), Muon ensures that every update is a mathematically pure step in the right direction, unhindered by the scale of the gradients.

## 6. Scope & Implementation Notes

Because the Muon optimizer operates on the principles of Orthogonalization, it is specifically designed for **2D matrices**.

1.  **Biases & Embeddings:** Muon is usually **not** applied to bias vectors or embedding layers (which are effectively lookup tables). These are typically trained with standard AdamW.
2.  **Convolutions:** A Convolutional filter has a 4D shape ($C_{out}, C_{in}, H, W$). To apply Muon, we **flatten** the tensor into a 2D matrix of shape ($C_{out}, C_{in} \times H \times W$). This treats the filter bank as a linear mapping from an input volume to an output pixel, orthogonalizing the filters relative to each other.

## 7. Code
### 1. The Newton-Schulz Function
This function performs the iterative orthogonalization on the momentum matrix.
```python
import torch

@torch.compile  # Optional: Compiles the function for faster execution on GPU
def zeropower_via_newtonschulz5(G, steps=5, eps=1e-7):
    """
    Newton-Schulz iteration to compute the best orthogonal approximation of G.
    The coefficients are carefully tuned to push singular values towards 1.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750, 2.0315)
    
    # Enable bfloat16 for speed if available, otherwise float32
    dtype = G.dtype
    if G.device.type != 'cpu' and dtype != torch.float16: 
         # bfloat16 is preferred for stability over float16
        G = G.to(torch.bfloat16)

    # 1. Normalize the matrix to ensure spectral norm <= 1
    # We want to orthogonalize the rows, so we normalize by the largest row norm equivalent
    # (Here we use Frobenius norm as a safe proxy for spectral scaling)
    X = G / (G.norm() + eps)
    
    # 2. Transpose if necessary to ensure we orthogonalize the larger dimension
    # (Optimization trick: Work on the smaller covariance matrix)
    if G.size(0) > G.size(1):
        X = X.T

    # 3. Newton-Schulz Iteration
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X
    
    # 4. Restore original shape if transposed
    if G.size(0) > G.size(1):
        X = X.T

    return X.to(dtype)
```

### 2. The Muon Optimizer Class
Note that Muon requires Nesterov momentum logic internally to apply the orthogonalization correctly to the "lookahead" gradient.
```python
import torch.optim as optim

class Muon(optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-Schulz
    
    Args:
        params: 2D Parameters to optimize (Linear weights, Conv2d kernels reshaped).
        lr: Learning rate (default: 0.02).
        momentum: Momentum factor (default: 0.95).
        nesterov: Whether to use Nesterov momentum (default: True).
        ns_steps: Number of Newton-Schulz iterations (default: 5).
        weight_decay: Weight decay factor (default: 0.01).
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 ns_steps=5, weight_decay=0.01):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov,
                        ns_steps=ns_steps, weight_decay=weight_decay)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            weight_decay = group['weight_decay']

            for p in group['params']:
                if p.grad is None:
                    continue
                
                g = p.grad
                
                # 0. Flatten 4D tensors (Conv2D) to 2D
                # Muon only works on matrices.
                original_shape = p.shape
                if p.ndim > 2:
                    g = g.view(g.size(0), -1)
                    p_flat = p.view(p.size(0), -1)
                else:
                    p_flat = p

                state = self.state[p]

                # 1. State Initialization
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)

                buf = state['momentum_buffer']
                
                # 2. Apply Weight Decay (AdamW style - decoupled)
                if weight_decay != 0:
                    p_flat.mul_(1 - lr * weight_decay)

                # 3. Update Momentum
                buf.mul_(momentum).add_(g)

                # 4. Nesterov Lookahead (Crucial for Muon)
                # We orthogonalize the "future" direction
                if nesterov:
                    update_matrix = g + momentum * buf
                else:
                    update_matrix = buf

                # 5. Orthogonalize the Update Matrix (Newton-Schulz)
                # This ensures the update has uniform spectral energy
                update_matrix = zeropower_via_newtonschulz5(update_matrix, steps=ns_steps)

                # 6. Scale the update
                # We scale by max(1, rows/cols)^0.5 to keep variance consistent 
                # across different layer shapes.
                row_col_ratio = update_matrix.size(0) / update_matrix.size(1)
                scale_factor = max(1.0, row_col_ratio) ** 0.5
                
                update_matrix.mul_(scale_factor)

                # 7. Apply Update
                # Note: We reshape update_matrix back to original shape if needed
                if p.ndim > 2:
                    p.add_(update_matrix.view(original_shape), alpha=-lr)
                else:
                    p.add_(update_matrix, alpha=-lr)

        return loss
```
### 3. How to Use (Crucial!)
Muon is specialized. It should only be used for the large interior matrices (Linear/Conv layers). You must continue to use AdamW for embeddings, normalization scales, and biases.


Here is a helper function to split your model parameters automatically:

```python
def get_muon_params(model):
    """
    Separates parameters into:
    1. Muon params: Hidden 2D weights (Linear, Conv)
    2. AdamW params: Embeddings, Norms, Biases, 1D vectors
    """
    muon_params = []
    adamw_params = []

    for name, p in model.named_parameters():
        # Exclude embeddings and final classification heads usually
        # Also exclude scalars/vectors (biases, layer norms)
        if p.ndim >= 2 and 'embed' not in name and 'head' not in name:
            muon_params.append(p)
        else:
            adamw_params.append(p)
            
    return muon_params, adamw_params

# --- Usage in Training Loop ---

# 1. Get param groups
muon_p, adamw_p = get_muon_params(model)

# 2. Define Optimizers
# Note: Muon LR is often higher than AdamW (e.g., 0.02 vs 0.001)
opt_muon = Muon(muon_p, lr=0.02, momentum=0.95)
opt_adam = torch.optim.AdamW(adamw_p, lr=0.001, weight_decay=0.01)

# 3. Step both optimizers
loss.backward()
opt_muon.step()
opt_adam.step()
opt_muon.zero_grad()
opt_adam.zero_grad()
```

### References:
- [YouTube: This Simple Optimizer Is Revolutionizing How We Train AI [Muon]](https://www.youtube.com/watch?v=bO5nvE289ec)
- [Muon: An optimizer for hidden layers in neural networks](https://kellerjordan.github.io/posts/muon/)
- [Muon Pytorch Implementation](https://github.com/KellerJordan/Muon/blob/857d1adc159e098212f828d927460782d3da59e9/muon.py#L138)
- [Understanding the Muon Optimizer: Theory and Implementation](https://huggingface.co/datasets/bird-of-paradise/muon-tutorial)
- Blog is generated with the help of Gemini.